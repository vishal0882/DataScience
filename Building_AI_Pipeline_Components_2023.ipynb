{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishal0882/DataScience/blob/main/Building_AI_Pipeline_Components_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04O2oSfyl6Pr"
      },
      "source": [
        "<center><img src=\"https://gitlab.com/accredian/insaid-data/-/raw/main/Logo-Accredian/Case-Study-Cropped.png\" width= 30% /></center>\n",
        "\n",
        "# <center>**Building AI Pipeline Components**</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5RKNiirmGJR"
      },
      "source": [
        "----\n",
        "# **Table of Contents**\n",
        "----\n",
        "\n",
        "1. [**Introduction to Vertex AI**](#section1)<br>\n",
        "2. [**Dataset Description**](#section2)<br>\n",
        "3. [**Data Preprocessing**](#section3)<br>\n",
        "4. [**Model Building using AutoML**](#section401)<br>\n",
        "5. [**Model Deployment**](#section5)<br>\n",
        "6. [**Make Prediction**](#section6)<br>\n",
        "9. [**Conclusion**](#section9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=section1></a>\n",
        "\n",
        "---\n",
        "# **1. Vertex AI Pipelines**\n",
        "---\n",
        "\n",
        "- We will be using **Vertex AI to build a pipeline** for the AI solution, starting from Data collection to cleaning, model building, deployment and evaluation.\n",
        "\n",
        "- Vertex AI supports both **KFP**(Kubeflow Piplines) and **TFX**(Tensorflow Extended) depending on the **usage and the requirement of a project**.\n",
        "\n",
        "- So if you're already using Tensorflow, then **TFX** is the good choice for you. It provides a rich set of component that helps you take the TensorFlow code and make it into an **ML pipeline**.\n",
        "\n",
        "- Now, if you're **not using TensorFlow**, then use **KFP**, which is an **open-source machine learning pipeline**.It offers a great deal of **flexibility**. It's easy to plug in code from **any ML framework**, including the normal frameworks that aren't Python-based (like Apache MXNet)\n",
        "\n",
        "- So **why should we use Vertex AI Pipelines, if we can use KFP directly?** The best answer to this FAQ is that **Vertex Pipelines are Managed**, which means you don't have to maintain and create servers by yourself. When you're **using KFP**, you need to **build, scale, and maintain a Kubernetes cluster**, all by yourself. But with managed pipelines, we don't have to do any of that work. It's all **serverless**.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://miro.medium.com/v2/resize:fit:828/0*HJP9KWImrg-1mn61\"/></center>\n",
        "\n",
        "<br>  "
      ],
      "metadata": {
        "id": "efdpef5h2WKD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgUvIk-GCyQ5"
      },
      "source": [
        "<a id=section2></a>\n",
        "\n",
        "---\n",
        "# **2. Dataset Description**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The **Titanic dataset** is a popular dataset in the field of data science and machine learning.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://www.roomrecess.com/Stories/Titanic/TitanicMain.png\" /></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- It contains information about **passengers who were aboard the RMS Titanic when it sank on its maiden voyage in April 1912**.\n",
        "\n",
        "- This dataset is often used for predictive modeling and data analysis tasks, such as **predicting whether a passenger survived or not based on various features**. The dataset typically includes the following columns.\n",
        "\n",
        "- Our task is to create a **classification model** in order to predict if a passenger is going to survive or not.\n",
        "\n",
        "- You can access the dataset from [**here**](https://gitlab.com/accredian/insaid-data/-/raw/main/titanic.csv?ref_type=heads).\n",
        "\n",
        "<br>  \n",
        "\n",
        "|Feature name|Feature description|\n",
        "|:--|:--|\n",
        "**PassengerID** | A unique identifier for each passenger in the dataset   \n",
        "**Survived** | Target column indicating if the passenger survived (1) or not (0)\n",
        "**PClass** | Class of the ticket the passenger purchased.  \n",
        "**Name** | Name of the passenger\n",
        "**Sex**  | Gender of the passenger   \n",
        "**Age**   |   Age of the passenger   \n",
        "**SibSp** |  Number of siblings or spouses the passenger had aboard the Titanic     \n",
        "**Parch** |  Number of parents or children the passenger had aboard the Titanic   \n",
        "**Ticket** | Number of pounds gained by the mother during pregnancy.  \n",
        "**Fare** | TThe fare or price paid for the ticket.   \n",
        "**Cabin** | The cabin number where the passenger stayed\n",
        "**Embarked** | The port of embarkation for the passenger(Cherbourg, Queenstown or Southampton)"
      ],
      "metadata": {
        "id": "RWZ4GKRy2mvB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHBtQli6mST2"
      },
      "source": [
        "<a id=section3></a>\n",
        "\n",
        "---\n",
        "# **3. Data Preprocessing**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Prerequisites**\n",
        "\n",
        "- First, navigate to [**Google Cloud Console**](https://console.cloud.google.com/) and **Create a new Project** or you can just continue working in the project you were working in.\n",
        "\n",
        "- Make sure that **billing** is **enabled** for the cloud project that you're working in.\n",
        "\n",
        "- As we've already established, GCP uses **storage buckets** to store the data, therefore naviagte to **Google Cloud Storage** and create storage **Bucket**. This is what we will use to get data for our pipeline.\n",
        "\n",
        "- You can **get the dataset** we are using in this project [**here**](https://gitlab.com/accredian/insaid-data/-/raw/main/titanic.csv?ref_type=heads) and simply create a storage bucket.\n",
        "\n",
        "- Alternatively, you can also create a folder for storing your clean files in this bucket itself but it's **completely optional**.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1WsFmwVvXHc00EJJdDPOXV3xNOfupvCIm\"/></center>\n",
        "\n",
        "<br>  "
      ],
      "metadata": {
        "id": "b5A60HDRDleg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- When we get the data from anywhere, there are a lot od issues like, missing values, inconsistencies etc. this is why preprocessing is an essential part of the entire workflow.\n",
        "\n",
        "- The entire objective of **making a pipeline** is to make all this function **automated** so that we don't have to do it over and over again.\n",
        "\n",
        "- This is why we are going to use **Dataprep** for the preprocessing and data preparation."
      ],
      "metadata": {
        "id": "BLUYTjJQI5Cb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataprep:**\n",
        "\n",
        "- Dataprep is an **intelligent data service** for visually exploring, cleaning, and preparing **structured** and **unstructured** data for analysis, reporting, and machine learning.\n",
        "\n",
        "- Because Dataprep is **serverless** and works at any scale, there is no **infrastructure** to deploy or manage.\n",
        "\n",
        "- Some other **advantages** that **Google Cloud Dataprep** are :\n",
        "\n",
        "   - It will be easy to **use and integrate** other services provided by Google Cloud Platform.\n",
        "\n",
        "   - People can access **multiple data sources** from Cloud Storage and BigQuery for further analysis.\n",
        "\n",
        "   - The prepared data can also be used by services like **Google Data Studio** or Google Cloud **Machine Learning Engine** to train ML models and analysis.\n",
        "\n",
        "   - There is **no need for any VM**.\n",
        "\n",
        "   - The process becomes much easier with the **intuitive GUI** which lets you see the whole process.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/e2e/pd39.png\" width=\"900\"height = \"400\"/></center>\n",
        "\n",
        "<br>  "
      ],
      "metadata": {
        "id": "5fo4mlQaHJf9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Search for **Dataprep** and navigate to **Dataprep from GCP Console**.\n",
        "\n",
        "- Using it for the first time, you will be asked to set up a **default storage location** for your preprocessed files. nYou can set it according to your need or go with the default.\n",
        "\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1M917b5D76tTzuzH_HXEx7lNNRST99XLX\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- Dataprep consists of a lot of **elements**, aiming to make your preprocessing effective and easy.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=13OP91Y8HxTjC5NcXwQTUEO5kRZS9eHN5\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "- In Dataprep, we create **Flows**. A flow in Dataprep is kind of a **container object that brings together and organizes datasets, recipes, and other objects used to generate results**. Flows are used to access and manipulate datasets, and they can be created to organize datasets, recipes, and imported reference objects.\n",
        "- Working in a **team**, many users can **collaborate on the same flow objects** in real-time or create copies for others to use for independent work.\n",
        "\n",
        "- The **Connections** are an important aspect of Dataprep. 99% of the times you pull data from a **database** instead of a csv, **Bigquery** can help with the connection to a database from where the data can be queried and pulled regularly from continously updated data. For this project we will be using the **cloud storage bucket** that we created.\n",
        "\n",
        "- To create a flow in simply, **click the Flows icon**, then click Create and select Create Flow. Fill in a flow name and description, then click **Create**.\n",
        "\n",
        "- You will be directed to a flow which looks something like this, simply click on **connect to your data** to get started.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1HhnCRzcCMbvxUpxAowNclJZXM8tE3Wrt\" /></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- After connecting you will get an option to **import your datasets**. We will import ours from the bucket that we created earlier. This is the step where you can import data from multiple sources, depending on your project.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1iwjNZ8F4x-qCyZbtjhy_rvUW4vySYzrK\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1bQFlKbxOWE3L5PagYwV8t3a_H7RCh3yA\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1_HxYk4TG6tiA_q_f34hicxvaysjPccuf\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "- You can add as many files you want in a bucket. This helps with the data which is constantly updated and added to the database.\n",
        "\n",
        "-  On the right hand side it shows a **little preview of the data** before you import it and then simply **add the data to your flow**.\n"
      ],
      "metadata": {
        "id": "4Bk6RwiiCO6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now you can see you've moved a step up in your flow and can see the option of **Recipe**.\n",
        "\n",
        "- A **recipe** in Dataprep is a **sequence of steps** that are applied to imported datasets to **transform them for output**. Recipes are used to **clean, transform, and enrich data by applying a series of operations**, such as filtering, splitting, and joining, to the dataset etc. Dataprep even allows custom functions in order to edit the data according to your needs.\n",
        "\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1gkYGWuvsI4uv3FrPAzxsy935sf-n5IFD\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- Simply click on edit recipe and you'll be taken to the data preprocessing page. To draw an analogy it's similar to the pandas profiling that we do, and **gives information and distribution of the columns and highlights all the irregularities**.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1Pj_-dEr4lZYhcJg6apmxFN0vjngp5Lfw\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- You can explore the data in order to understand what changes and **transformations needs to be performed** and then you can work around a recipe for it, simply click on edit recipe and add your desired steps.\n",
        "\n",
        "- In this dataset **we've performed a few transformations**, you can work **according to your needs**.\n",
        "\n",
        "- When you're satisfied with your recipe, **click on run** and you'll be directed to a page to save this data.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1GtJGijuQLU11bBt5pmuwFuWHe-hfl2cZ\" /></center>\n",
        "\n",
        "<br>  \n"
      ],
      "metadata": {
        "id": "fWEFkZLgCjoi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataprep provides **2 ways** of running environment. **Trifacta Photon** which is usually best for running jobs on **small datasets**, which is by Trifacta itself and **Dataflow** which is usually done for very large datasets that we're pulling from **Databases**.\n",
        "\n",
        "- We will be using **Dataflow in our project** as it helps us to **visualise our flow**, and make the process much easier to get a birds-eye view on.\n",
        "\n",
        "- Lets talk about **Jobs**. Basically in Dataprep, **a job is a unit of work that is executed on a dataset or a recipe**. Jobs can be used to perform various tasks, **such as running a recipe on a dataset**, publishing results to another datastore, or tracking changes to a dataset.\n",
        "\n",
        "- They can be executed on-demand or on a schedule, and they can be monitored and managed from the **Jobs page in the Dataprep UI**.\n",
        "\n",
        "- Dataprep provides various job types, such as **recipe jobs, dataset jobs**, and **flow jobs**, each with its own set of parameters and options.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=13hRh8KlgjT9AL_C4ADwt1H99ltRegz03\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "- You can choose to **change how and where you want to save the data**. In this project we've saved the location of the preprocessed file in the same bucket where the data is present. You can do the same or anything **depending on your needs**.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1odIN0V5SBnj4eUHvai1G8LOx9H9sR6-6\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "- After you're satisfied with the settings, simply **click on Run** and your preprocessed data will be stored and ready to be modelled. It'll take **some time** to be processed and then you can work around your tasks.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1r9Nym2iqct32KlD48KcsSn8lLO6cmH5K\" /></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=11EvvtVi-kGJFsnIomYeiqsoA90IvepLK\" /></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "- You can see the **summary of the jobs** with execution details along with all the necessary information. You can even **download this file in your local computer**.\n",
        "\n"
      ],
      "metadata": {
        "id": "Zr8d-CQVJ4AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Now it's the time to explore the really exciting feature of **visualising the flow** that we created.\n",
        "\n",
        "- In your GCP console go to [**Dataflow**](https://console.cloud.google.com/dataflow/jobs?referrer=search&authuser=1&hl=en&project=auto-vision-nlp-341507) and **navigate to the Jobs section**, **or simply click on the view dataflow job** in the top right corner of your Dataprep console. Here you'll be able to see the jobs that you've created.\n",
        "\n",
        "- Dataflow provides **complete map of execution** of each job. Also, we can **visualize the usage of resources** and **logs**.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1GEIflt7ZSvldKXECFUOW_ge6xqM07y1H\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- **Click on the job** to visualize and you'll be able to see the **entire flow**, along with all the creation steps and their details.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=13Wmz7gwEULZvMhGlGAuHXxqFrX3nn8IO\" /></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- This flow that we created **can be scheduled** as well according to the data that we are working on.**For example**, suppose the database that you're working with gets dumped with new data at 9AM every morning, scheduling this flow will help you to perform all these processes **automatically**. You won't need to come and run it everyday.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gSaOgrCkClKg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Ii1GjMHNPk"
      },
      "source": [
        "<a id=section4></a>\n",
        "\n",
        "---\n",
        "# **4. Model Building using AutoML**\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we are aiming for an automation pipeline, **AutoML** will be a good choice for the **data modeling** bit.\n",
        "\n",
        "- We've already been through AutoML and we'll do the same thing here as well. As the dataset is **binary classification** type we will use the **Tabular Data** option in the AutoML portion.\n",
        "\n",
        "- Therefore, simply go to [**Vertex AI datasets**](https://console.cloud.google.com/vertex-ai/datasets) section, select **tabular dat**a and click on **create**.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=15ffHJkfvBlti3hdKuHAj4zgB7Q5YCz_Q\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- Provide source of **stored dataset**. We will be using the cloud storage option where we have stored the preprocessed data.\n",
        "\n",
        "-Simply **provide the bucket location of your preprocessed data** (we saved ours in teh dataprep folder of our dataset bucket earlier) and you'll be able to create the dataset for modelling.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1jFL1ilyjdF3R8XhzfHlVkT6kVpLYwe9i\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- Click on the dataset and you'll be able to see all the **information regarding the data**. Click on Train Model to start with the model training.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=11Rc0i7h8-IL6dcACPFkcp1beGYR5yroj\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "- Click on train a new model and as we're choosing **AutoML, select that option**. You can also create a custom model, based on your data and requirements.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1Mf6Urtwa3mVrtSUfRiO70D5N2shMGGTO\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- Select the **model name**, and the **target column**.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1z-UT0tdHE1wraFsA5cpV8KjyHmzeAbnX\"/></center>\n",
        "\n",
        "- In the next setting, you have the option to **explore the dataset again** and make some **changes in the columns** if that transformation was not done or something extra is needed.\n",
        "\n",
        "- For **optimisation**, there are a lot of options to choose from depending upon the objective of your data. As we are dealing with **binary classification**, we chose the **Log Loss** options which gives prediction probabilities as well.\n",
        "\n",
        "<br>  \n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=15NWIVwNMFKwGr2nN9u2xCrq81lqXbt3h\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1uNXeoSa2tfsSUdIPZ9ZKShnMvRwVfjFV\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- Now simply, set the **node hours as 1 and start training**.\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1KBia7r7ZPai6Ccwuya0BHcB4tavDkqgT\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qe_7gIyTCoCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The training can take upto **2-3 hours** to complete.\n",
        "\n",
        "- After completion we can see the **feature importance** as well as training **result**.\n",
        "\n",
        "- All the graphs, matrices and **model evaluation results** are available after the model is trained.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1s99RMhxprQniTOKJeSDKO3rMpuShHyMG\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n"
      ],
      "metadata": {
        "id": "Wld27JK2Cpd6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JeX11RTVxjS"
      },
      "source": [
        "<a id=section5></a>\n",
        "\n",
        "---\n",
        "# **5. Model Deployment**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can also **export** this model as a **tensorflow package**.\n",
        "\n",
        "- Provide suitable **gcp bucket storage address** to export the model.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://raw.githubusercontent.com/insaid2018/DeepLearning/master/e2e/pd24.png\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- To **deploy** the model navigate to **Deploy and Test** option.\n",
        "\n",
        "- Click on **Deploy to endpoint**.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1i-ke-i7QKDlslMW_NvYZfaxiXdvpyQc-\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- Provide endpoint name and access preference.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1Z3Y1v15bgRYPsCl9OMvbOevl7RtinJC0\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "- Whenever working with **tabular data**, it's important to include feature attributions in the explainability options, while deploying the model. Feature attributions can be used to explain the **contribution of each feature in the data** to the expected outcome. This information can be used to **check that the model is working as expected** and to gain a **better understanding** of how the model makes decisions.\n",
        "\n",
        "- By **default**, this option is **disabled**, click on **Feature Attribution** and continue. **Failing to do so will give you a prediction error after the model is deployed.**\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1IY5sw_e0P-c4K7w-TOQTJ3eKKNljnhyt\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "- Go on with the **default settings** and in the monitoring part provide the **dataset information** we're working on (the preprocessed data). After everything is stablished click on **Deploy** and your model will be deployed to an endpoint.\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1dHCaUIwNY_2CYrP5PFcS21RO9XOgai72\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1NQOjNiYD44OqKIvruGNYoiIX4hnDt6KJ\"/></center>\n",
        "\n",
        "<br>  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sfVLw0z8SV1q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvJEJRL7xUPn"
      },
      "source": [
        "<a id=section6></a>\n",
        "\n",
        "---\n",
        "# **6. Make Prediction**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Model deployment can **take some time**.\n",
        "\n",
        "- After deployment you will be able to **test your model** with the get the results and make preictions\n",
        "\n",
        "<br>  \n",
        "<center><img src = \"https://drive.google.com/uc?export=download&id=1gck-Y3sa0xXSCFTSy-yShjv4AHCgbjkF\"/></center>\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "buqq7r8TSgoG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvFjhPZwWo7B"
      },
      "source": [
        "<a id=section7></a>\n",
        "\n",
        "---\n",
        "# **7. Conclusion**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tools **designed** for the line of business offer users a new **workflow and experience** for working with data.\n",
        "\n",
        "- Even the **technical layman** can generate **impactful insight** and create **ML Models**.\n",
        "\n",
        "- The top goal of **data prep adopters** is to\n",
        "improve the **overall quality** of data and analysis. Users expect data\n",
        "prep tools to help them **work faster** and **perform more advanced** analytics.\n",
        "\n",
        "- AutoML is rapidly **democratizing machine learning** tools and boosting productivity, as it enables machine learning engineers, data scientists, data analysts, and even **non-technical users** to **automate repetitive and manual** machine **learning tasks**.\n"
      ],
      "metadata": {
        "id": "l_EVn99UwkSi"
      }
    }
  ]
}